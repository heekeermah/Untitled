# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NMGU9W4j26MObTZ29yOXg793u2b6eCFS
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

df = pd.read_csv('DiseaseAndSymptoms.csv')

df.head()

df.info()

# prompt: remove the columns from symptom_4 to symptom_17

cols_to_drop = [f'Symptom_{i}' for i in range(4, 18)]
df = df.drop(columns=cols_to_drop)
df.head()
df.info()

# prompt: duplicates

# Count duplicate rows
duplicate_rows = df.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_rows}")

# Display duplicate rows
if duplicate_rows > 0:
    print("\nDuplicate rows:")
    print(df[df.duplicated(keep=False)]) # keep=False marks all duplicates as True

# Remove duplicate rows
df_no_duplicates = df.drop_duplicates()
print(f"\nShape of DataFrame after removing duplicates: {df_no_duplicates.shape}")

df.head()

# prompt: Encode the column symptoms

import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
df['Symptoms'] = df[['Symptom_1', 'Symptom_2', 'Symptom_3']].apply(lambda row: [s for s in row if pd.notna(s)], axis=1)
encoded_symptoms = mlb.fit_transform(df['Symptoms'])

# Create new columns for each symptom
symptom_cols = [f'Symptom_{i+1}' for i in range(encoded_symptoms.shape[1])]
encoded_df = pd.DataFrame(encoded_symptoms, columns=symptom_cols)

# Concatenate the original dataframe with the encoded symptoms dataframe
df = pd.concat([df.drop(columns=['Symptom_1', 'Symptom_2', 'Symptom_3', 'Symptoms']), encoded_df], axis=1)

df.head()
df.info()

# prompt: train and test the dataset

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

X = df.drop('Disease', axis=1)
y = df['Disease']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train a model (Logistic Regression as an example)
model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Print classification report for more detailed evaluation
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

!pip install streamlit

import streamlit as st


st.title("AI Health Assistant")
symptoms = st.multiselect("Select your symptoms", X.columns)
input_data = [1 if s in symptoms else 0 for s in X.columns]

if st.button("Predict"):
    prediction = model.predict([input_data])
    st.success(f"Possible diagnosis: {prediction[0]}")

# prompt: run this as an app with the neccesary libraries

# The code provided is a good starting point, but cannot be directly run
# as a Streamlit app within a standard Colaboratory cell.
# Streamlit apps require their own server process.

# To run this as a Streamlit app in Colaboratory, you would typically:
# 1. Save the Streamlit code to a Python file (e.g., app.py).
# 2. Run this file using `streamlit run app.py` in a terminal.
# Since Colab doesn't provide a persistent terminal in the standard notebook cell,
# we need to use a workaround to run the Streamlit server.

# Install necessary libraries (if not already installed in the Colab environment)
!pip install -q streamlit pandas scikit-learn

import streamlit as st
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import sys
import os

# --- Model Training (Place this logic outside the Streamlit app definition) ---
# In a real app, you would likely train the model once and load it.
# For this example within Colab, we'll include the training here,
# but be aware this will re-train every time the Streamlit server restarts.

# Define the file upload mechanism. This part is tricky in Colab/Streamlit.
# We'll assume the file is already uploaded or exists in a known path
# for the Streamlit app to access it.
# For this example, let's simulate loading from a file path.
# If you're running this locally or via a service like ngrok,
# the `files.upload()` part from the original Colab code wouldn't work directly.
# A typical Streamlit way is `st.file_uploader`.
# Let's adapt the code to read from a potential file path first.

# Assume the dataset is named 'DiseaseAndSymptoms.csv' and is in the same directory
# as the Python script we will create for the Streamlit app.
# If you are running this in Colab, you'll need to upload the file manually
# or access it from Drive.

# Let's create a temporary Python file for the Streamlit app code
streamlit_code = """
import streamlit as st
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import os

# Define the path to your dataset. Adjust this path if needed.
# If running in Colab, make sure the file is accessible.
DATASET_PATH = 'DiseaseAndSymptoms.csv'

# --- Model Training Function ---
# This function will train the model.
# We wrap it in a function and use caching if possible (though caching
# might not be straightforward with file uploads).
# For simplicity here, we'll just train directly.

try:
    df = pd.read_csv(DATASET_PATH)

    # Data Cleaning and Preprocessing
    cols_to_drop = [f'Symptom_{i}' for i in range(4, 18)]
    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])

    # Remove duplicate rows
    df = df.drop_duplicates()

    # Multi-Label Encoding for Symptoms
    mlb = MultiLabelBinarizer()
    # Ensure the columns exist before applying the function
    symptom_cols_exist = [col for col in ['Symptom_1', 'Symptom_2', 'Symptom_3'] if col in df.columns]
    if symptom_cols_exist:
        df['Symptoms'] = df[symptom_cols_exist].apply(lambda row: [s for s in row if pd.notna(s)], axis=1)
        encoded_symptoms = mlb.fit_transform(df['Symptoms'])

        # Create new columns for each symptom
        symptom_col_names = [f'Symptom_{i+1}' for i in range(encoded_symptoms.shape[1])]
        encoded_df = pd.DataFrame(encoded_symptoms, columns=symptom_col_names, index=df.index)

        # Concatenate the original dataframe with the encoded symptoms dataframe
        # Drop the original symptom columns and the temporary 'Symptoms' column
        cols_to_drop_final = symptom_cols_exist + ['Symptoms']
        df = pd.concat([df.drop(columns=[col for col in cols_to_drop_final if col in df.columns]), encoded_df], axis=1)
    else:
        # Handle case where symptom columns don't exist (though based on original code, they should)
        st.error("Symptom columns (Symptom_1, Symptom_2, Symptom_3) not found in the dataset.")
        df = pd.DataFrame() # Empty dataframe to prevent further errors

    # Model Training
    if not df.empty and 'Disease' in df.columns:
        X = df.drop('Disease', axis=1)
        y = df['Disease']

        # Check if X is empty after preprocessing
        if X.empty:
             st.error("Feature set is empty after preprocessing. Cannot train model.")
             model = None
             mlb = None # Reset mlb if training fails
             X_columns = []
        else:
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            model = LogisticRegression(max_iter=1000)
            model.fit(X_train, y_train)

            # Store the columns used for training to ensure consistent input format
            X_columns = X.columns.tolist()

            # Optional: Print evaluation metrics (can be removed for production app)
            # y_pred = model.predict(X_test)
            # accuracy = accuracy_score(y_test, y_pred)
            # st.write(f"Model Training Accuracy: {accuracy:.2f}")
            # st.write("Classification Report:")
            # st.text(classification_report(y_test, y_pred))

    else:
        st.error("Dataset could not be loaded or 'Disease' column is missing.")
        model = None
        mlb = None # Reset mlb if training fails
        X_columns = []


except FileNotFoundError:
    st.error(f"Error: Dataset file not found at {DATASET_PATH}. Please upload 'DiseaseAndSymptoms.csv' or place it in the correct directory.")
    model = None
    mlb = None
    X_columns = []
except Exception as e:
    st.error(f"An error occurred during data loading or model training: {e}")
    model = None
    mlb = None
    X_columns = []


# --- Streamlit App ---
st.title("AI Health Assistant")

if model is not None and mlb is not None:
    st.write("Model loaded and ready.")

    # Create the multiselect widget using the columns from the trained model's features (X)
    # Ensure we only show columns that are boolean symptom flags
    possible_symptoms = [col for col in X_columns if col.startswith('Symptom_')]
    selected_symptoms = st.multiselect("Select your symptoms", possible_symptoms)

    if st.button("Predict"):
        if not selected_symptoms:
            st.warning("Please select at least one symptom.")
        else:
            # Create the input vector for the model
            # It must have the same number of features and in the same order as X_columns
            input_data_dict = {col: 0 for col in X_columns}
            for sym in selected_symptoms:
                if sym in input_data_dict:
                    input_data_dict[sym] = 1

            # Convert dictionary to a list in the correct order
            input_data = [input_data_dict[col] for col in X_columns]

            try:
                # Make prediction
                prediction = model.predict([input_data]) # model expects a 2D array
                st.success(f"Possible diagnosis: {prediction[0]}")
            except Exception as e:
                st.error(f"An error occurred during prediction: {e}")

elif model is None:
    st.warning("Model could not be trained. Please check the dataset file and contents.")

"""

# Write the Streamlit code to a file
with open("app.py", "w") as f:
    f.write(streamlit_code)

# To run this, you would typically use:
# !streamlit run app.py & npx localtunnel --port 8501
# The `&` sends the streamlit process to the background.
# `npx localtunnel --port 8501` creates a public URL to access the app running on port 8501.
# You need to install localtunnel if you haven't already.

# Install localtunnel
!npm install -g localtunnel

# Run Streamlit in the background and expose it via localtunnel
# Check if a streamlit process is already running and kill it
# This helps avoid port conflicts if you run the cell multiple times
!pgrep streamlit | xargs kill -9

# Make sure the dataset file exists before running Streamlit
# If running in Colab, you need to upload 'DiseaseAndSymptoms.csv'
# Example: If you manually uploaded it to the default content directory
if not os.path.exists('DiseaseAndSymptoms.csv'):
    print("Please upload 'DiseaseAndSymptoms.csv' to the current directory before running the Streamlit app.")
    print("You can use the file browser on the left pane in Colab to upload.")
else:
    print("Dataset found. Starting Streamlit app...")
    # Run the Streamlit app and create a tunnel
    # The output will include a public URL you can click.
    # It might ask for confirmation in the terminal (y/n).
    # Since we can't interact easily, we'll use a simple approach.
    # Note: This command block might keep the cell running until you interrupt it.
    # You'll find the public URL in the output.
    !streamlit run app.py &>/dev/null& npx localtunnel --port 8501


# You should see a public URL printed in the output (e.g., https://[random-id].loca.lt).
# Click on this URL to access your Streamlit app.
# If it shows a "This site can’t be reached" or similar error, try running the cell again.
# If it asks "Would you like to expose...", type 'y' in the implicit terminal and press Enter.
# Since we used `&>/dev/null&`, you might not see the prompt, but the tunnel might establish anyway.
# If it doesn't work, you might need a more robust tunneling solution or run this locally.

!pip install pyngrok==6.0.0

from pyngrok import ngrok
import time

    # Authenticate with ngrok (Optional but recommended for stability)
    # You can get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken
    # Replace 'YOUR_AUTHTOKEN' with your actual authtoken
    # ngrok.set_auth_token("YOUR_AUTHTOKEN")

    # Give Streamlit a moment to start
time.sleep(10) # Adjust sleep time if needed

    # Assuming Streamlit is running on port 8501
try:
        public_url = ngrok.connect(port='8501').public_url
        print(f"Streamlit app is available at: {public_url}")
except Exception as e:
        print(f"Error creating ngrok tunnel: {e}")
        print("Please ensure your Streamlit app is running on port 8501.")
        print("Check the output of the Streamlit run cell for details.")